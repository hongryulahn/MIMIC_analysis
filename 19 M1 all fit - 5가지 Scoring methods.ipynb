{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68873e8d",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39b8e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 결과 확인을 용이하게 하기 위한 코드\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "#한글설정\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_dirs = ['/usr/share/fonts/truetype/nanum', ]\n",
    "font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    fm.fontManager.addfont(font_file)\n",
    "    \n",
    "# 한글 출력을 위해서 폰트 옵션을 설정합니다.\n",
    "# \"axes.unicode_minus\" : 마이너스가 깨질 것을 방지\n",
    "\n",
    "sns.set(font=\"NanumBarunGothic\", \n",
    "        rc={\"axes.unicode_minus\":False},\n",
    "        style='darkgrid')\n",
    "\n",
    "# GPU 용량 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:  # gpu가 있다면, 용량 한도를 5GB로 설정\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[1], \n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10*1024)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62767695",
   "metadata": {},
   "source": [
    "# 함수 모음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fce8b",
   "metadata": {},
   "source": [
    "## RF_정확도(itemlist, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05c8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_정확도(itemlist,name):\n",
    "\n",
    "    # itemlist만을 가진 x_(7727,10)_itemlist 만들기\n",
    "    item_list = list(total_data['ITEMID'].sort_values().unique())\n",
    "    \n",
    "    item_index = []\n",
    "    for i in itemlist:\n",
    "        item_index.append(item_list.index(i))\n",
    "\n",
    "    print('itemlist의 index : ',item_index)\n",
    "\n",
    "    x = np.load('x_(7727,4068).npy')\n",
    "    x_2d = x[:,item_index]\n",
    "    x_2d\n",
    "    \n",
    "    np.save(f'x_(7727,10)_{name}.npy',x_2d)\n",
    "    \n",
    "    import random\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    data={}\n",
    "    for seed in range(42, 52):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        x = np.load(f'./x_(7727,10)_{name}.npy')\n",
    "        y = np.load('./y_(7727,1).npy')\n",
    "\n",
    "        idx = list(range(len(x)))\n",
    "        random.shuffle(idx)\n",
    "\n",
    "        i = round(x.shape[0]*0.8)\n",
    "        X_train, y_train = x[idx[:i],:], y[idx[:i]]\n",
    "        X_test, y_test = x[idx[i:],:], y[idx[i:]]\n",
    "\n",
    "        _ = model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred_test)\n",
    "        data[seed]=acc\n",
    "        print(f'정확도 : {acc}, seed_num = {seed}')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    print(f'정확도 df 만들고 평균 확인 : {df.mean().values}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd36c2",
   "metadata": {},
   "source": [
    "## LSTM_정확도(itemlist, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e46ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "\n",
    "def LSTM_정확도(itemlist, name):\n",
    "\n",
    "    # itemlist만을 가진 x_(7727,10,10)_itemlist 만들기\n",
    "    total_data = pd.read_csv('total_data_7727.csv')\n",
    "    item_list = list(total_data['ITEMID'].sort_values().unique())\n",
    "\n",
    "    item_index = []\n",
    "    for i in itemlist:\n",
    "        item_index.append(item_list.index(i))\n",
    "\n",
    "    print('itemlist의 index : ',item_index)\n",
    "\n",
    "    x = np.load('x_(7727,10,4068).npy')\n",
    "    x_3d = x[:,:,item_index]\n",
    "    \n",
    "    np.save(f'x_(7727,10,10)_{name}.npy',x_3d)\n",
    "    \n",
    "    seed_num = 42\n",
    "    random.seed(seed_num)\n",
    "    \n",
    "    x = np.load(f'x_(7727,10,10)_{name}.npy')\n",
    "    y = np.load('y_(7727,1).npy')\n",
    "\n",
    "    idx = list(range(len(x)))\n",
    "    random.shuffle(idx)\n",
    "\n",
    "    i = round(x.shape[0]*0.8)\n",
    "    X_train, y_train = x[idx[:i],:,:], y[idx[:i]]\n",
    "    X_test, y_test = x[idx[i:],:,:], y[idx[i:]]\n",
    "\n",
    "    # ---------------------\n",
    "    seed_num = 42 \n",
    "    # ---------------------\n",
    "    tf.random.set_seed(seed_num)\n",
    "\n",
    "    lstm = Sequential()\n",
    "    lstm.add(InputLayer(input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    lstm.add(LSTM(units=128, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "    lstm.add(LSTM(units=32, activation='hard_sigmoid', return_sequences=False))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "    import os\n",
    "\n",
    "    MODEL_SAVE_FOLDER_PATH = f'./model/{name}'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + f'/{name}_seed42-'+'{epoch:02d}'+'-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                                    verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=50, verbose=1, restore_best_weights=True)\n",
    "    lstm.compile(optimizer= keras.optimizers.Adam(learning_rate = 0.001), loss = \"binary_crossentropy\", metrics=['acc'])\n",
    "    lstm.fit(X_train, y_train, validation_split=0.25, batch_size=128, epochs=500,  callbacks=[early_stop,cb_checkpoint], shuffle=False)\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5*1024)])\n",
    "\n",
    "        from pathlib import Path\n",
    "        paths = sorted(Path(MODEL_SAVE_FOLDER_PATH).iterdir(), key=os.path.getmtime)[-1]\n",
    "        best_model_path = str(paths)\n",
    "\n",
    "        from keras.models import load_model\n",
    "        best_model = load_model(best_model_path) \n",
    "\n",
    "        dic_42={}\n",
    "        for seed in range(0, 50):\n",
    "            random.seed(seed)\n",
    "\n",
    "            x = np.load(f'x_(7727,10,10)_{name}.npy')\n",
    "            y = np.load('y_(7727,1).npy')\n",
    "\n",
    "            idx = list(range(len(x)))\n",
    "            random.shuffle(idx)\n",
    "\n",
    "            i = round(x.shape[0]*0.8)\n",
    "            X_train, y_train = x[idx[:i],:,:], y[idx[:i]]\n",
    "            X_test, y_test = x[idx[i:],:,:], y[idx[i:]]\n",
    "\n",
    "            pred = best_model.predict(X_test)\n",
    "            pred[pred>0.5]=1\n",
    "            pred[pred<=0.5]=0\n",
    "            acc = metrics.accuracy_score(y_test, pred)\n",
    "            dic_42[seed]=acc\n",
    "            print(f'정확도 :{metrics.accuracy_score(y_test, pred)}, seed_num = {seed}')\n",
    "\n",
    "        df_42 = pd.DataFrame.from_dict(dic_42, orient='index')\n",
    "        print(f'정확도 df 만들고 평균 확인 : {df_42.mean().values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514946e",
   "metadata": {},
   "source": [
    "## absum(item_list)\n",
    "- 18에 비해서 hyperparameter 1개로 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95975a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_csv('total_data_7727.csv')\n",
    "x = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "\n",
    "def absum(itemlist):\n",
    "    # 1) PRE_top10의 index 구하기\n",
    "    item_list = total_data['ITEMID'].sort_values().unique()\n",
    "\n",
    "    PPL_index = []\n",
    "    for i in itemlist:\n",
    "        a = list(item_list).index(i)\n",
    "        PPL_index.append(a)\n",
    "    \n",
    "    # 2) 생존자 index 구하기 \n",
    "    sub7727 = total_data['SUBJECT_ID'].unique()\n",
    "\n",
    "    patient = pd.read_csv('폐렴환자.csv')\n",
    "    patient = patient.sort_values(by='SUBJECT_ID')\n",
    "    patient = patient[patient['SUBJECT_ID'].isin(sub7727)]\n",
    "    sub_1_list = patient[patient['EXPIRE_FLAG']==0]['SUBJECT_ID'].values\n",
    "\n",
    "    생존자_index = []\n",
    "    for i in sub_1_list:\n",
    "        생존자_index.append(list(sub7727).index(i))\n",
    "        \n",
    "    # 3) 생존자 3009명의 D-10 ~ D-1 feature별 abnormal sum \n",
    "    x_생존자 = x[생존자_index,:,:]\n",
    "\n",
    "    result1 = []\n",
    "    for i in PPL_index:\n",
    "        for j in range(10):\n",
    "            result1.append(x_생존자[:,j,i].sum())\n",
    "\n",
    "    result1 = np.array(result1)\n",
    "    result1 = result1.reshape(10,-1)\n",
    "    df_PPL = pd.DataFrame(result1)\n",
    "    df_PPL.columns = [f'D-{i}' for i in range(10,0,-1)]\n",
    "    df_PPL.index = itemlist\n",
    "\n",
    "    # 생존한 4718명에 대한 비율 계산\n",
    "    for i in itemlist:\n",
    "        df_PPL.loc[f'{i}_생존'] = df_PPL.loc[i].iloc[:]/x_생존자.shape[0]\n",
    "        \n",
    "    # 4) 사망자 4718명의 D-10 ~ D-1 feature별 abnormal sum \n",
    "    사망자_index = list(set(range(0,7727))-set(생존자_index))\n",
    "\n",
    "    x_사망자 = x[사망자_index,:,:]\n",
    "\n",
    "    result2 = []\n",
    "    for i in PPL_index:\n",
    "        for j in range(10):\n",
    "            result2.append(x_사망자[:,j,i].sum())\n",
    "\n",
    "    result2 = np.array(result2)\n",
    "    result2 = result2.reshape(10,-1)\n",
    "    df_PPL_사망자 = pd.DataFrame(result2)\n",
    "    df_PPL_사망자.columns = [f'D-{i}' for i in range(10,0,-1)]\n",
    "    df_PPL_사망자.index = itemlist\n",
    "\n",
    "    # 사망한 3009명에 대한 비율 계산\n",
    "    for i in itemlist:\n",
    "        df_PPL_사망자.loc[f'{i}_사망'] = df_PPL_사망자.loc[i]/x_사망자.shape[0] \n",
    "        \n",
    "    # 5) for문으로 각 feature에 대한 비율 추이 그래프 그리기 \n",
    "\n",
    "    df_PPL_trans = df_PPL.transpose()\n",
    "    df_PPL_사망자_trans = df_PPL_사망자.transpose()\n",
    "\n",
    "    _ = plt.figure(figsize = (13,12),dpi=150)\n",
    "    for i, f in enumerate(itemlist): \n",
    "        _ = plt.subplot(4,3,1+i)\n",
    "        _ = plt.title(f)\n",
    "        _ = ax = sns.lineplot(data = df_PPL_사망자_trans, x = df_PPL_사망자_trans.index, y = f'{f}_사망')\n",
    "        _ = ax = sns.lineplot(data = df_PPL_trans, x = df_PPL_trans.index, y = f'{f}_생존')\n",
    "        _ = ax.legend(labels = ['사망', '생존'], loc = 'upper left', fontsize=12)\n",
    "        _ = ax.set_ylabel('per', fontsize = 12)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda669cf",
   "metadata": {},
   "source": [
    "## violin_allfit(itemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa970540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_allfit(itemlist):\n",
    "    \n",
    "    # 1) DATA \n",
    "    x = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "    y = np.load('/project/LSH/y_(7727,1).npy')\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5*1024)])\n",
    "        # 2) MODEL\n",
    "        from keras.models import load_model\n",
    "        lstm2 = load_model('./model/allfit2_seed42-06-0.5519.hdf5')\n",
    "\n",
    "        _ = plt.figure(figsize = (13,12),dpi=150)\n",
    "\n",
    "        for j, itemid in tqdm(enumerate(itemlist)):\n",
    "\n",
    "            # 3) 환자별 base pred (Y')\n",
    "            base_pred = pd.DataFrame(lstm2.predict(x))\n",
    "\n",
    "            total_data = pd.read_csv('total_data_7727.csv')\n",
    "            features = total_data['ITEMID'].sort_values().unique()\n",
    "\n",
    "            k = list(features).index(itemid)\n",
    "            save_col = x[:,:,k].copy()\n",
    "\n",
    "            # 4) 0 to 1\n",
    "            x[:,:,k]= np.where(x[:,:,k]==0, 1, x[:,:,k])\n",
    "            pred_0to1 = pd.DataFrame(lstm2.predict(x))\n",
    "\n",
    "            # 5) 1 to 0\n",
    "            x[:,:,k] = np.where(x[:,:,k]==1, 0, x[:,:,k])\n",
    "            pred_1to0 = pd.DataFrame(lstm2.predict(x))\n",
    "\n",
    "            # 6) inverse\n",
    "            x[:,:,k] = save_col\n",
    "            x[:,:,k] = np.where(x[:,:,k]==1, 2, x[:,:,k])\n",
    "            x[:,:,k] = np.where(x[:,:,k]==0, 1, x[:,:,k])\n",
    "            x[:,:,k] = np.where(x[:,:,k]==2, 0, x[:,:,k])\n",
    "            pred_inverse = pd.DataFrame(lstm2.predict(x))\n",
    "\n",
    "            x[:,:,k] = save_col\n",
    "\n",
    "            # 7) Merge & Visualize\n",
    "            FI_merge_df = pd.concat([base_pred, pred_0to1, pred_1to0, pred_inverse], axis=1)\n",
    "            FI_merge_df.columns = ['base_pred', 'pred_0to1', 'pred_1to0', 'pred_inverse']\n",
    "            FI_merge_visual = FI_merge_df.melt(value_vars=['base_pred','pred_0to1','pred_1to0','pred_inverse'])\n",
    "            FI_merge_visual.columns = ['method','pred_value']\n",
    "\n",
    "            _ = plt.subplot(4,3,1+j)\n",
    "            _ = plt.title(itemid)\n",
    "            ax = sns.violinplot(data=FI_merge_visual, x='method',y='pred_value',\n",
    "                                palette=\"Set2\", inner=\"quartile\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f6bfc",
   "metadata": {},
   "source": [
    "# 1️⃣ m1_allfit\n",
    "- feature별 `E(0to1)-E(1to0)`이 담긴 df 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10600003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2708/4068 [1:18:53<39:37,  1.75s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4384b26e3acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 3) E(0to1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mpred_0to1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 4) E(1to0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1) DATA \n",
    "import random    \n",
    "seed_num = 42\n",
    "random.seed(seed_num)\n",
    "\n",
    "x = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "y = np.load('/project/LSH/y_(7727,1).npy')\n",
    "\n",
    "total_data = pd.read_csv('total_data_7727.csv')\n",
    "features = total_data['ITEMID'].sort_values().unique()\n",
    "\n",
    "# 2) MODEL\n",
    "from keras.models import load_model\n",
    "lstm2 = load_model('./model/allfit2_seed42-06-0.5519.hdf5')\n",
    "\n",
    "results = []\n",
    "with tf.device('/device:GPU:1'):\n",
    "    for i in tqdm(range(len(features))):\n",
    "\n",
    "        save_col = x[:,:,i].copy()\n",
    "\n",
    "        # 3) E(0to1)\n",
    "        x[:,:,i] = np.where(x[:,:,i]==0, 1, x[:,:,i])\n",
    "        pred_0to1 = np.mean(lstm2.predict(x, batch_size=10000, workers=-1, use_multiprocessing=True))\n",
    "\n",
    "        # 4) E(1to0)\n",
    "        x[:,:,i] = np.where(x[:,:,i]==1, 0, x[:,:,i])\n",
    "        pred_1to0 = np.mean(lstm2.predict(x, batch_size=10000, workers=-1, use_multiprocessing=True))\n",
    "\n",
    "        x[:,:,i] = save_col\n",
    "\n",
    "        # 5) Merge\n",
    "        mean_diff = pred_0to1 - pred_1to0\n",
    "        results.append({'feature':features[i],'mean_diff':mean_diff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('m1_allfit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d710170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd50aba0",
   "metadata": {},
   "source": [
    "# 2️⃣ m1_allfit_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.read_csv('m1_entropy.csv')\n",
    "entropy = entropy.sort_values(by='feature')\n",
    "entropy.index = range(4068)\n",
    "\n",
    "m1_allfit = pd.read_csv('m1_allfit.csv')\n",
    "m1_allfit_entropy = m1_allfit.copy()\n",
    "m1_allfit_entropy['diff*entropy'] = entropy['entropy'] * m1_allfit['mean_diff']\n",
    "m1_allfit_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddf9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f3d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58f98bbc",
   "metadata": {},
   "source": [
    "# 3️⃣ m1_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c50211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4e6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334c4007",
   "metadata": {},
   "source": [
    "# 4️⃣ m1_sequential_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf1b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
